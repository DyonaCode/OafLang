#!/usr/bin/env bash
set -euo pipefail

OUTPUT_PATH="${1:-benchmarks/CI_BENCHMARKS.md}"
BENCH_ITERATIONS="${BENCH_ITERATIONS:-100}"
KERNEL_ITERATIONS="${KERNEL_ITERATIONS:-5}"
SUM_N="${SUM_N:-5000000}"
PRIME_N="${PRIME_N:-30000}"
MATRIX_N="${MATRIX_N:-48}"
CROSS_LANG_ITERATIONS="${CROSS_LANG_ITERATIONS:-5}"

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
OUTPUT_FILE="${ROOT_DIR}/${OUTPUT_PATH}"

mkdir -p "$(dirname "${OUTPUT_FILE}")"

TMP_DIR="$(mktemp -d)"
trap 'rm -rf "${TMP_DIR}"' EXIT

BENCHMARK_OUTPUT="${TMP_DIR}/benchmark_output.txt"
KERNEL_OUTPUT="${TMP_DIR}/kernel_output.csv"
CROSS_LANG_STDOUT="${TMP_DIR}/cross_lang_output.txt"
CROSS_LANG_CSV="${TMP_DIR}/cross_lang.csv"

run_or_dump() {
  local label="$1"
  local output_file="$2"
  shift 2
  if ! "$@" > "${output_file}" 2>&1; then
    echo "[benchmark-report] ${label} failed. Captured output:" >&2
    cat "${output_file}" >&2 || true
    exit 1
  fi
}

run_or_dump "throughput benchmark" "${BENCHMARK_OUTPUT}" \
  dotnet run --configuration Release --no-build -- --benchmark "${BENCH_ITERATIONS}"

run_or_dump "kernel benchmark" "${KERNEL_OUTPUT}" \
  dotnet run --configuration Release --no-build -- --benchmark-kernels \
  --iterations "${KERNEL_ITERATIONS}" \
  --sum-n "${SUM_N}" \
  --prime-n "${PRIME_N}" \
  --matrix-n "${MATRIX_N}"

run_or_dump "cross-language benchmark" "${CROSS_LANG_STDOUT}" \
  bash ./scripts/benchmark/run_c_rust_benchmarks.sh \
  --iterations "${CROSS_LANG_ITERATIONS}" \
  --sum-n "${SUM_N}" \
  --prime-n "${PRIME_N}" \
  --matrix-n "${MATRIX_N}" \
  --oaf-mode both \
  --out "${CROSS_LANG_CSV}"

if [[ ! -s "${CROSS_LANG_CSV}" ]]; then
  echo "[benchmark-report] cross-language benchmark did not produce CSV output at ${CROSS_LANG_CSV}." >&2
  cat "${CROSS_LANG_STDOUT}" >&2 || true
  exit 1
fi

sanitize_output() {
  sed '/^CSSM_ModuleLoad()/d' "$1"
}

GENERATED_AT="$(date -u +"%Y-%m-%d %H:%M:%SZ")"
COMMIT_SHA="$(git -C "${ROOT_DIR}" rev-parse --short HEAD)"

cat > "${OUTPUT_FILE}" <<EOF
# CI Benchmark Report

This file is generated by \`.github/workflows/benchmark-report.yml\`.

- Generated (UTC): ${GENERATED_AT}
- Commit: \`${COMMIT_SHA}\`
- Runner OS: \`linux\`

## Throughput Benchmarks

Command:
\`\`\`bash
dotnet run --configuration Release --no-build -- --benchmark ${BENCH_ITERATIONS}
\`\`\`

\`\`\`text
$(sanitize_output "${BENCHMARK_OUTPUT}")
\`\`\`

## Kernel Benchmarks

Command:
\`\`\`bash
dotnet run --configuration Release --no-build -- --benchmark-kernels --iterations ${KERNEL_ITERATIONS} --sum-n ${SUM_N} --prime-n ${PRIME_N} --matrix-n ${MATRIX_N}
\`\`\`

\`\`\`csv
$(sanitize_output "${KERNEL_OUTPUT}")
\`\`\`

## Cross-Language Benchmarks (C, Rust, Oaf VM, Oaf Native)

Command:
\`\`\`bash
./scripts/benchmark/run_c_rust_benchmarks.sh --iterations ${CROSS_LANG_ITERATIONS} --sum-n ${SUM_N} --prime-n ${PRIME_N} --matrix-n ${MATRIX_N} --oaf-mode both
\`\`\`

Raw CSV:
\`\`\`csv
$(sanitize_output "${CROSS_LANG_CSV}")
\`\`\`

Relative to C baseline (mean_ms ratio):

| Algorithm | Rust/C | Oaf VM/C | Oaf Native/C |
|---|---:|---:|---:|
$(awk -F, '
NR == 1 { next }
{
  key = $2
  mean[$1, key] = $5 + 0.0
  alg[key] = 1
}
END {
  for (key in alg) {
    c = mean["c", key]
    if (c <= 0) {
      continue
    }

    rust = (("rust", key) in mean) ? sprintf("%.3fx", mean["rust", key] / c) : "n/a"
    vm = (("oaf_vm", key) in mean) ? sprintf("%.3fx", mean["oaf_vm", key] / c) : "n/a"
    exe = (("oaf_exe", key) in mean) ? sprintf("%.3fx", mean["oaf_exe", key] / c) : "n/a"
    printf "| %s | %s | %s | %s |\n", key, rust, vm, exe
  }
}' "${CROSS_LANG_CSV}" | sort)

## Notes

- Values can fluctuate between runs due to shared CI runner variability.
- Treat this report as trend/reference data, not a strict performance guarantee.
EOF
